{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55fb3264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.9 (main, Dec  7 2022, 13:15:23) [GCC 9.4.0]\n",
      "2.0.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn.metrics as skm\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "from datasets import Dataset\n",
    "from tokenizers import *\n",
    "from tokenizers.processors import BertProcessing\n",
    "import json\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(sys.version)\n",
    "print(torch.__version__)\n",
    "\n",
    "# maximum sequence length\n",
    "max_length = 16 # note in paper, 128 used\n",
    "\n",
    "# pad to max_length\n",
    "def repeat_first_and_last(lst):\n",
    "    first_element = lst[0]\n",
    "    last_element = lst[-1]\n",
    "    return [first_element] + lst + [last_element]\n",
    "\n",
    "def adjust_visit_ids(ser):\n",
    "    min_value = ser[0]\n",
    "    adjusted_vis = [v - min_value + 1 for v in ser]\n",
    "    return adjusted_vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3286f15",
   "metadata": {},
   "source": [
    "# Read in and format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6b0a755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patid</th>\n",
       "      <th>code_list</th>\n",
       "      <th>age_ids</th>\n",
       "      <th>visit_ids</th>\n",
       "      <th>year_ids</th>\n",
       "      <th>gender_ids</th>\n",
       "      <th>eth_ids</th>\n",
       "      <th>imd_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[421235014, 275301017, 82343012, 396742015, 25...</td>\n",
       "      <td>[10, 10, 11, 11, 11]</td>\n",
       "      <td>[1, 1, 2, 3, 3]</td>\n",
       "      <td>[2, 2, 3, 3, 3]</td>\n",
       "      <td>[2, 2, 2, 2, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>[5, 5, 5, 5, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>[405621000000116, 281181000006116, 303867016, ...</td>\n",
       "      <td>[2, 3, 4, 4, 5]</td>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>[1, 5, 6, 6, 12]</td>\n",
       "      <td>[2, 2, 2, 2, 2]</td>\n",
       "      <td>[4, 4, 4, 4, 4]</td>\n",
       "      <td>[9, 9, 9, 9, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>[368051000006111, 303392010, 288711015, 167359...</td>\n",
       "      <td>[2, 3, 4, 4, 5]</td>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>[1, 5, 6, 6, 12]</td>\n",
       "      <td>[1, 1, 1, 1, 2]</td>\n",
       "      <td>[5, 5, 5, 5, 5]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>[184656013, 906061000006116, 969331000006115]</td>\n",
       "      <td>[10, 10, 11]</td>\n",
       "      <td>[1, 1, 2]</td>\n",
       "      <td>[2, 2, 3]</td>\n",
       "      <td>[2, 2, 2]</td>\n",
       "      <td>[6, 6, 6]</td>\n",
       "      <td>[2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>[348110010]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[8]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>[19794011, 19794011, 19794011, 645331000006112]</td>\n",
       "      <td>[12, 12, 13, 13]</td>\n",
       "      <td>[1, 2, 3, 4]</td>\n",
       "      <td>[9, 10, 11, 12]</td>\n",
       "      <td>[2, 2, 2, 2]</td>\n",
       "      <td>[3, 3, 3, 3]</td>\n",
       "      <td>[3, 3, 3, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>[1484866013, 736361000006117, 411198017, 40562...</td>\n",
       "      <td>[6, 6, 7, 7, 7, 8, 9, 9]</td>\n",
       "      <td>[1, 1, 2, 2, 3, 4, 5, 5]</td>\n",
       "      <td>[2, 2, 3, 3, 3, 4, 7, 7]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4]</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>[304875018, 1786700015, 253940015, 253940015, ...</td>\n",
       "      <td>[2, 3, 4, 4, 5]</td>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>[1, 5, 6, 6, 12]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>[5, 5, 5, 5, 5]</td>\n",
       "      <td>[6, 6, 6, 6, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>[294870013, 294603013, 2871720012, 31102100000...</td>\n",
       "      <td>[10, 10, 11, 11, 11]</td>\n",
       "      <td>[1, 1, 2, 3, 3]</td>\n",
       "      <td>[2, 2, 3, 3, 3]</td>\n",
       "      <td>[2, 2, 2, 2, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>[8, 8, 8, 8, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>[136211012]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[8]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8</td>\n",
       "      <td>[1673591000006119, 84541000006111, 1484866013,...</td>\n",
       "      <td>[12, 12, 13, 13]</td>\n",
       "      <td>[1, 2, 3, 4]</td>\n",
       "      <td>[9, 10, 11, 12]</td>\n",
       "      <td>[2, 2, 2, 2]</td>\n",
       "      <td>[2, 2, 2, 2]</td>\n",
       "      <td>[1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9</td>\n",
       "      <td>[304875018, 891311000006118, 891301000006116, ...</td>\n",
       "      <td>[6, 6, 7, 7, 7, 8, 9, 9]</td>\n",
       "      <td>[1, 1, 2, 2, 3, 4, 5, 5]</td>\n",
       "      <td>[2, 2, 3, 3, 3, 4, 7, 7]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3]</td>\n",
       "      <td>[10, 10, 10, 10, 10, 10, 10, 10]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patid                                          code_list  \\\n",
       "0      1  [421235014, 275301017, 82343012, 396742015, 25...   \n",
       "1     10  [405621000000116, 281181000006116, 303867016, ...   \n",
       "2     11  [368051000006111, 303392010, 288711015, 167359...   \n",
       "3     12      [184656013, 906061000006116, 969331000006115]   \n",
       "4      2                                        [348110010]   \n",
       "5      3    [19794011, 19794011, 19794011, 645331000006112]   \n",
       "6      4  [1484866013, 736361000006117, 411198017, 40562...   \n",
       "7      5  [304875018, 1786700015, 253940015, 253940015, ...   \n",
       "8      6  [294870013, 294603013, 2871720012, 31102100000...   \n",
       "9      7                                        [136211012]   \n",
       "10     8  [1673591000006119, 84541000006111, 1484866013,...   \n",
       "11     9  [304875018, 891311000006118, 891301000006116, ...   \n",
       "\n",
       "                     age_ids                 visit_ids  \\\n",
       "0       [10, 10, 11, 11, 11]           [1, 1, 2, 3, 3]   \n",
       "1            [2, 3, 4, 4, 5]           [1, 2, 3, 4, 5]   \n",
       "2            [2, 3, 4, 4, 5]           [1, 2, 3, 4, 5]   \n",
       "3               [10, 10, 11]                 [1, 1, 2]   \n",
       "4                        [1]                       [1]   \n",
       "5           [12, 12, 13, 13]              [1, 2, 3, 4]   \n",
       "6   [6, 6, 7, 7, 7, 8, 9, 9]  [1, 1, 2, 2, 3, 4, 5, 5]   \n",
       "7            [2, 3, 4, 4, 5]           [1, 2, 3, 4, 5]   \n",
       "8       [10, 10, 11, 11, 11]           [1, 1, 2, 3, 3]   \n",
       "9                        [1]                       [1]   \n",
       "10          [12, 12, 13, 13]              [1, 2, 3, 4]   \n",
       "11  [6, 6, 7, 7, 7, 8, 9, 9]  [1, 1, 2, 2, 3, 4, 5, 5]   \n",
       "\n",
       "                    year_ids                gender_ids  \\\n",
       "0            [2, 2, 3, 3, 3]           [2, 2, 2, 2, 2]   \n",
       "1           [1, 5, 6, 6, 12]           [2, 2, 2, 2, 2]   \n",
       "2           [1, 5, 6, 6, 12]           [1, 1, 1, 1, 2]   \n",
       "3                  [2, 2, 3]                 [2, 2, 2]   \n",
       "4                        [8]                       [1]   \n",
       "5            [9, 10, 11, 12]              [2, 2, 2, 2]   \n",
       "6   [2, 2, 3, 3, 3, 4, 7, 7]  [1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "7           [1, 5, 6, 6, 12]           [1, 1, 1, 1, 1]   \n",
       "8            [2, 2, 3, 3, 3]           [2, 2, 2, 2, 2]   \n",
       "9                        [8]                       [1]   \n",
       "10           [9, 10, 11, 12]              [2, 2, 2, 2]   \n",
       "11  [2, 2, 3, 3, 3, 4, 7, 7]  [1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "\n",
       "                     eth_ids                           imd_ids  \n",
       "0            [1, 1, 1, 1, 1]                   [5, 5, 5, 5, 5]  \n",
       "1            [4, 4, 4, 4, 4]                   [9, 9, 9, 9, 9]  \n",
       "2            [5, 5, 5, 5, 5]                   [1, 1, 1, 1, 1]  \n",
       "3                  [6, 6, 6]                         [2, 2, 2]  \n",
       "4                        [2]                               [4]  \n",
       "5               [3, 3, 3, 3]                      [3, 3, 3, 3]  \n",
       "6   [4, 4, 4, 4, 4, 4, 4, 4]          [2, 2, 2, 2, 2, 2, 2, 2]  \n",
       "7            [5, 5, 5, 5, 5]                   [6, 6, 6, 6, 6]  \n",
       "8            [1, 1, 1, 1, 1]                   [8, 8, 8, 8, 8]  \n",
       "9                        [2]                               [7]  \n",
       "10              [2, 2, 2, 2]                      [1, 1, 1, 1]  \n",
       "11  [3, 3, 3, 3, 3, 3, 3, 3]  [10, 10, 10, 10, 10, 10, 10, 10]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in pkl file\n",
    "disease = pd.read_pickle('DUMMY_EHRBERT_DATA.pkl')\n",
    "print(len(disease))\n",
    "disease"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e77fd2",
   "metadata": {},
   "source": [
    "### Format data - truncation and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0fd6396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patid</th>\n",
       "      <th>code_list</th>\n",
       "      <th>age_ids</th>\n",
       "      <th>visit_ids</th>\n",
       "      <th>year_ids</th>\n",
       "      <th>gender_ids</th>\n",
       "      <th>eth_ids</th>\n",
       "      <th>imd_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>421235014 275301017 82343012 396742015 256478018</td>\n",
       "      <td>[10, 10, 10, 11, 11, 11, 11, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[1, 1, 1, 2, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[2, 2, 2, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>405621000000116 281181000006116 303867016 2592...</td>\n",
       "      <td>[2, 2, 3, 4, 4, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 2, 3, 4, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 5, 6, 6, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[9, 9, 9, 9, 9, 9, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>368051000006111 303392010 288711015 1673591000...</td>\n",
       "      <td>[2, 2, 3, 4, 4, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 2, 3, 4, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 5, 6, 6, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>184656013 906061000006116 969331000006115</td>\n",
       "      <td>[10, 10, 10, 11, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[1, 1, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[2, 2, 2, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>348110010</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>19794011 19794011 19794011 645331000006112</td>\n",
       "      <td>[12, 12, 12, 13, 13, 13, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 2, 3, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[9, 9, 10, 11, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>1484866013 736361000006117 411198017 405621000...</td>\n",
       "      <td>[6, 6, 6, 7, 7, 7, 8, 9, 9, 9, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 2, 2, 3, 4, 5, 5, 5, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[2, 2, 2, 3, 3, 3, 4, 7, 7, 7, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>304875018 1786700015 253940015 253940015 14848...</td>\n",
       "      <td>[2, 2, 3, 4, 4, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 2, 3, 4, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 5, 6, 6, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>294870013 294603013 2871720012 311021000006113...</td>\n",
       "      <td>[10, 10, 10, 11, 11, 11, 11, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[1, 1, 1, 2, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[2, 2, 2, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>136211012</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8</td>\n",
       "      <td>1673591000006119 84541000006111 1484866013 259...</td>\n",
       "      <td>[12, 12, 12, 13, 13, 13, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 2, 3, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[9, 9, 10, 11, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9</td>\n",
       "      <td>304875018 891311000006118 891301000006116 4615...</td>\n",
       "      <td>[6, 6, 6, 7, 7, 7, 8, 9, 9, 9, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 2, 2, 3, 4, 5, 5, 5, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[2, 2, 2, 3, 3, 3, 4, 7, 7, 7, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 0, 0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patid                                          code_list  \\\n",
       "0      1   421235014 275301017 82343012 396742015 256478018   \n",
       "1     10  405621000000116 281181000006116 303867016 2592...   \n",
       "2     11  368051000006111 303392010 288711015 1673591000...   \n",
       "3     12          184656013 906061000006116 969331000006115   \n",
       "4      2                                          348110010   \n",
       "5      3         19794011 19794011 19794011 645331000006112   \n",
       "6      4  1484866013 736361000006117 411198017 405621000...   \n",
       "7      5  304875018 1786700015 253940015 253940015 14848...   \n",
       "8      6  294870013 294603013 2871720012 311021000006113...   \n",
       "9      7                                          136211012   \n",
       "10     8  1673591000006119 84541000006111 1484866013 259...   \n",
       "11     9  304875018 891311000006118 891301000006116 4615...   \n",
       "\n",
       "                                              age_ids  \\\n",
       "0   [10, 10, 10, 11, 11, 11, 11, 0, 0, 0, 0, 0, 0,...   \n",
       "1    [2, 2, 3, 4, 4, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2    [2, 2, 3, 4, 4, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "3   [10, 10, 10, 11, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "4    [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5   [12, 12, 12, 13, 13, 13, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "6    [6, 6, 6, 7, 7, 7, 8, 9, 9, 9, 0, 0, 0, 0, 0, 0]   \n",
       "7    [2, 2, 3, 4, 4, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "8   [10, 10, 10, 11, 11, 11, 11, 0, 0, 0, 0, 0, 0,...   \n",
       "9    [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "10  [12, 12, 12, 13, 13, 13, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "11   [6, 6, 6, 7, 7, 7, 8, 9, 9, 9, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                           visit_ids  \\\n",
       "0   [1, 1, 1, 2, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1   [1, 1, 2, 3, 4, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2   [1, 1, 2, 3, 4, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "3   [1, 1, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4   [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5   [1, 1, 2, 3, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "6   [1, 1, 1, 2, 2, 3, 4, 5, 5, 5, 0, 0, 0, 0, 0, 0]   \n",
       "7   [1, 1, 2, 3, 4, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "8   [1, 1, 1, 2, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "9   [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "10  [1, 1, 2, 3, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "11  [1, 1, 1, 2, 2, 3, 4, 5, 5, 5, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                             year_ids  \\\n",
       "0    [2, 2, 2, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1   [1, 1, 5, 6, 6, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "2   [1, 1, 5, 6, 6, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "3    [2, 2, 2, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4    [8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5   [9, 9, 10, 11, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "6    [2, 2, 2, 3, 3, 3, 4, 7, 7, 7, 0, 0, 0, 0, 0, 0]   \n",
       "7   [1, 1, 5, 6, 6, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "8    [2, 2, 2, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "9    [8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "10  [9, 9, 10, 11, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "11   [2, 2, 2, 3, 3, 3, 4, 7, 7, 7, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                          gender_ids  \\\n",
       "0   [2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1   [2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2   [1, 1, 1, 1, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "3   [2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4   [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5   [2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "6   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]   \n",
       "7   [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "8   [2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "9   [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "10  [2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "11  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                             eth_ids  \\\n",
       "0   [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1   [4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2   [5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "3   [6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4   [2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5   [3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "6   [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0]   \n",
       "7   [5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "8   [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "9   [2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "10  [2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "11  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                              imd_ids  \n",
       "0    [5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1    [9, 9, 9, 9, 9, 9, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "2    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "3    [2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4    [4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "5    [3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "6    [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0]  \n",
       "7    [6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "8    [8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "9    [7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "10   [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "11  [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 0, 0,...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Truncate codes over max_len to max_len\n",
    "# using adjusted max_seq-length to 2 less than max length to account for addition of CLS and SEP\n",
    "\n",
    "var_list = ['code_list','age_ids','visit_ids','year_ids','gender_ids','eth_ids','imd_ids']\n",
    "max_seq_length = max_length-2\n",
    "\n",
    "for var in var_list:\n",
    "    disease[var] = disease[var].apply(lambda x: x[-max_seq_length:])\n",
    "    \n",
    "# Remove list from within code column:\n",
    "disease['code_list'] = [','.join(map(str, l)) for l in disease['code_list']]\n",
    "\n",
    "# Remove commas from within code column:\n",
    "disease['code_list'] = disease['code_list'].str.replace(',',' ')\n",
    "\n",
    "# Repeat first and last elements (to account for CLS and SEP tokens) then pad with zeros\n",
    "var_list = ['age_ids','visit_ids','year_ids','gender_ids','eth_ids','imd_ids']\n",
    "for var in var_list:\n",
    "    disease[var] = disease[var].apply(lambda x: repeat_first_and_last(x)) # repeating end elements to match [CLS] and [SEP]\n",
    "    disease[var] = disease[var].apply(lambda x: x + [0] * (max_length - len(x)) if len(x) < max_length else x[:max_length])\n",
    "\n",
    "# recode visit number/ID to start at 1 where relevant (e.g. if truncation results in starting at value >1)\n",
    "# Apply to visit_ids column\n",
    "disease['visit_ids'] = disease['visit_ids'].apply(adjust_visit_ids)\n",
    "assert len(disease.loc[disease['visit_ids'].apply(lambda x: x[0] > 1)]) == 0\n",
    "disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be1260fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['code_list', 'age_ids', 'visit_ids', 'year_ids', 'gender_ids', 'eth_ids', 'imd_ids', 'patid'],\n",
       "    num_rows: 12\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to dataset for transformer model\n",
    "disease.index=disease['patid']\n",
    "train_dataset = Dataset.from_pandas(pd.DataFrame(disease[['code_list','age_ids','visit_ids','year_ids',\n",
    "                                                   'gender_ids','eth_ids','imd_ids']]), preserve_index=True)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882ae273",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "Using WordLevel tokenizer (whole word) and add 5 special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afc47f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-24 13:05:34.756402: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-24 13:05:34.804247: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-24 13:05:35.695517: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006841897964477539,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 12,
       "unit": "ex",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90609819cb194b268e4b9752d64cb8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['code_list', 'age_ids', 'visit_ids', 'year_ids', 'gender_ids', 'eth_ids', 'imd_ids', 'patid', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "    num_rows: 12\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the dataset to dummy_train.txt to use as input for tokenizer\n",
    "\n",
    "def dataset_to_text(dataset, output_filename=\"data.txt\"):\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        for t in dataset[\"code_list\"]:\n",
    "            print(t, file=f)\n",
    "        \n",
    "dataset_to_text(train_dataset, \"dummy_train.txt\")\n",
    "files = [\"dummy_train.txt\"]\n",
    "\n",
    "# Train tokenizer\n",
    "tokenizer = Tokenizer(models.WordLevel(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "\n",
    "tokenizer.post_processor = BertProcessing((\"SEP\", 2), (\"CLS\", 1))\n",
    "\n",
    "special_tokens = [\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "trainer = trainers.WordLevelTrainer(special_tokens=special_tokens)\n",
    "\n",
    "tokenizer.train(files=files, trainer=trainer)\n",
    "\n",
    "# to use, need to use wrapped tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    # tokenizer_file=\"ehrbert_tokenizer.json\", # Can load directly, otherwise\n",
    "    bos_token=\"[CLS]\",\n",
    "    eos_token=\"[SEP]\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")\n",
    "wrapped_tokenizer.save_pretrained(\"ehrbert_tokenizer\")\n",
    "\n",
    "# Now apply, with truncation and padding\n",
    "def encode_with_truncation(examples):\n",
    "  \"\"\"Mapping function to tokenize the sentences passed with truncation\"\"\"\n",
    "  return wrapped_tokenizer(examples[\"code_list\"], truncation=True, padding=\"max_length\",\n",
    "                   max_length=max_length, return_special_tokens_mask=True)\n",
    "\n",
    "# tokenizing the train dataset - batched here set to false\n",
    "train_dataset = train_dataset.map(encode_with_truncation, batched=False)\n",
    "train_dataset.set_format(\"torch\")\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82e63f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['code_list', 'age_ids', 'visit_ids', 'year_ids', 'gender_ids', 'eth_ids', 'imd_ids', 'patid', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "     num_rows: 9\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['code_list', 'age_ids', 'visit_ids', 'year_ids', 'gender_ids', 'eth_ids', 'imd_ids', 'patid', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "     num_rows: 3\n",
       " }))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set train/test split:\n",
    "d = train_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "d[\"train\"], d[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "694a9977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "6\n",
      "13\n",
      "3\n",
      "7\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# Determine unique values for each variable\n",
    "AGE_SIZE = len({x for l in (disease['age_ids'].to_list()) for x in l})\n",
    "print(AGE_SIZE)\n",
    "VISIT_SIZE = len({x for l in (disease['visit_ids'].to_list()) for x in l})\n",
    "print(VISIT_SIZE)\n",
    "#assert VISIT_SIZE == (max_seq_length+1) # check should be max_len-2, plus 1 for 0 term (padding)\n",
    "YEAR_SIZE = len({x for l in (disease['year_ids'].to_list()) for x in l})\n",
    "print(YEAR_SIZE)\n",
    "GENDER_SIZE = len({x for l in (disease['gender_ids'].to_list()) for x in l})\n",
    "print(GENDER_SIZE)\n",
    "ETH_SIZE = len({x for l in (disease['eth_ids'].to_list()) for x in l})\n",
    "print(ETH_SIZE)\n",
    "IMD_SIZE = len({x for l in (disease['imd_ids'].to_list()) for x in l})\n",
    "print(IMD_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505eb367",
   "metadata": {},
   "source": [
    "### EHR-BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dce0cec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ehr_bert import EHRBertForMaskedLM, EHRBertModel, EHRBertEmbeddings, EHRBertConfig\n",
    "from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7144b5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EHR-BERT model configuration\n",
    "\n",
    "model_config = EHRBertConfig(vocab_size=wrapped_tokenizer.vocab_size, # default=30522\n",
    "                          max_position_embeddings=max_length, # default=512\n",
    "                          num_hidden_layers=6, # default=12\n",
    "                          num_attention_heads=12, # default=12\n",
    "                          hidden_size=288,    #default=768\n",
    "                          intermediate_size=512,   #default=3072\n",
    "                          age_size=AGE_SIZE,\n",
    "                          visit_size=VISIT_SIZE,\n",
    "                          gender_size=GENDER_SIZE,\n",
    "                          year_size=YEAR_SIZE,\n",
    "                          eth_size=ETH_SIZE,\n",
    "                          imd_size=IMD_SIZE,\n",
    "                          segment_include=False,\n",
    "                          position_include=False\n",
    "                         )\n",
    "\n",
    "model = EHRBertForMaskedLM(config=model_config)\n",
    "\n",
    "# create data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=wrapped_tokenizer, mlm=True, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c01e01cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2532838/2416666458.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  acc = load_metric('accuracy')\n"
     ]
    }
   ],
   "source": [
    "# define accuracy metric during training\n",
    "from datasets import load_metric\n",
    "acc = load_metric('accuracy')\n",
    "prec = load_metric('precision')\n",
    "f1 = load_metric(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    labels = eval_pred.label_ids\n",
    "    predictions = eval_pred.predictions[0]\n",
    "\n",
    "    indices = [[i for i, x in enumerate(labels[row]) if x != -100] for row in range(len(labels))]\n",
    "    labels = [labels[row][indices[row]] for row in range(len(labels))]\n",
    "    labels = [item for sublist in labels for item in sublist]\n",
    "    predictions = [predictions[row][indices[row]] for row in range(len(predictions))]\n",
    "    predictions = [item for sublist in predictions for item in sublist]\n",
    "\n",
    "    results1 = acc.compute(predictions=predictions, references=labels)\n",
    "    results3 = prec.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    results5 = f1.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    \n",
    "    return {\"Accuracy\": results1[\"accuracy\"],\n",
    "            \"Precision-weighted\": results3[\"precision\"],\n",
    "            \"F1-weighted\": results5[\"f1\"]}\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    This function reduces GPU memory overload.\n",
    "    \"\"\"\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    return pred_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b16ffd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set epochs and step size - num_evals = number of evaluations over all steps\n",
    "batch_size = 1\n",
    "grad_steps = 1\n",
    "num_evals = 5\n",
    "epochs = 10\n",
    "\n",
    "eval_steps = int(((np.floor((len(train_dataset)/batch_size)/grad_steps))*epochs)/num_evals)\n",
    "eval_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02360724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='ehrbert',          # path to save model checkpoint\n",
    "    evaluation_strategy=\"steps\",   # evaluate each `logging_steps` steps, or at epoch\n",
    "    overwrite_output_dir=True,      \n",
    "    num_train_epochs=epochs,        \n",
    "    per_device_train_batch_size=batch_size, \n",
    "    gradient_accumulation_steps=grad_steps, \n",
    "    per_device_eval_batch_size=batch_size,   \n",
    "    learning_rate=3e-5,            # default = 5e-5; BEHRT uses 3e-5, MedBERT uses 5e-5\n",
    "    eval_accumulation_steps = 1000,  \n",
    "    logging_steps=eval_steps,      # evaluate every X steps\n",
    "    save_steps=eval_steps,          \n",
    "    fp16=True                       \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a24413f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "# initialize the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=d[\"train\"],\n",
    "    eval_dataset=d[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics = preprocess_logits_for_metrics, # this ensures only output relevant tensors\n",
    "    tokenizer = wrapped_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbe175f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `EHRBertForMaskedLM.forward` and have been ignored: special_tokens_mask, code_list, patid. If special_tokens_mask, code_list, patid are not expected by `EHRBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/data/master/DS211/users/tb1009/venv310/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 9\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 90\n",
      "  Number of trainable parameters = 3891404\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [90/90 00:03, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision-weighted</th>\n",
       "      <th>F1-weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.664200</td>\n",
       "      <td>3.434896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>2.180100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>3.129400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `EHRBertForMaskedLM.forward` and have been ignored: special_tokens_mask, code_list, patid. If special_tokens_mask, code_list, patid are not expected by `EHRBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3\n",
      "  Batch size = 1\n",
      "/data/master/DS211/users/tb1009/venv310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ehrbert/checkpoint-24\n",
      "Configuration saved in ehrbert/checkpoint-24/config.json\n",
      "Model weights saved in ehrbert/checkpoint-24/pytorch_model.bin\n",
      "tokenizer config file saved in ehrbert/checkpoint-24/tokenizer_config.json\n",
      "Special tokens file saved in ehrbert/checkpoint-24/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `EHRBertForMaskedLM.forward` and have been ignored: special_tokens_mask, code_list, patid. If special_tokens_mask, code_list, patid are not expected by `EHRBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3\n",
      "  Batch size = 1\n",
      "/data/master/DS211/users/tb1009/venv310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ehrbert/checkpoint-48\n",
      "Configuration saved in ehrbert/checkpoint-48/config.json\n",
      "Model weights saved in ehrbert/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in ehrbert/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in ehrbert/checkpoint-48/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `EHRBertForMaskedLM.forward` and have been ignored: special_tokens_mask, code_list, patid. If special_tokens_mask, code_list, patid are not expected by `EHRBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3\n",
      "  Batch size = 1\n",
      "/data/master/DS211/users/tb1009/venv310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ehrbert/checkpoint-72\n",
      "Configuration saved in ehrbert/checkpoint-72/config.json\n",
      "Model weights saved in ehrbert/checkpoint-72/pytorch_model.bin\n",
      "tokenizer config file saved in ehrbert/checkpoint-72/tokenizer_config.json\n",
      "Special tokens file saved in ehrbert/checkpoint-72/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=90, training_loss=2.8057572258843315, metrics={'train_runtime': 4.0196, 'train_samples_per_second': 22.39, 'train_steps_per_second': 22.39, 'total_flos': 33377875200.0, 'train_loss': 2.8057572258843315, 'epoch': 10.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff8fd68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "venv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
